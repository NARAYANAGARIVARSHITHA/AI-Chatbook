# -*- coding: utf-8 -*-
"""Copy of project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NWd2cRIKwnYnDjy6UJlIUTDAD6uocbJt

Method 1
"""
import pandas as pd

from google.colab import drive
# Corrected the mountpoint to the standard /content/drive
drive.mount('/content/drive', force_remount=True)

# Assuming the file is in the root of your Google Drive.
# If it's in a different folder, update the path accordingly.

df = pd.read_excel("/content/drive/MyDrive/AI_PROJECT/Book1.xlsx")
df

import re
!pip install nltk
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

from sklearn.model_selection import train_test_split

# Replace NaNs with empty strings, then clean
df["Sentence"] = df["Sentence"].fillna("")
df["clean_text"] = df["Sentence"].apply(lambda x: re.sub('<.*?>', "", x))
"""df["clean_text"] = df["Sentence"].apply(lambda x: re.sub('<.*?>',"",x))"""

df

df["clean_text"] = df["clean_text"].apply(lambda x: re.sub(r'[^\w\s]',"",x))

df["clean_text"] =df["clean_text"].str.lower()

df["clean_text"]

import nltk
nltk.download("stopwords")

stop_words = set(stopwords.words("english"))

stop_words

nltk.download("punkt")
nltk.download("punkt_tab") 
df["tokenized_text"] = df["clean_text"].apply(lambda x: word_tokenize(x))

df["clean_text"]

df["filter_text"] = df["tokenized_text"].apply(lambda x: [word for word in x if word not in stop_words])

df["filter_text"]

stemmer = PorterStemmer()
#df["filter_text"] = df["filter_text"].apply(lambda x:[stemmer.stem(word) for word in x])

df["filter_text"]

from nltk.stem import WordNetLemmatizer
lamatize = WordNetLemmatizer()

nltk.download("wordnet")
df["filter_text"] = df["filter_text"].apply(lambda x:[lamatize.lemmatize(word) for word in x])

df["filter_text"]

X_train,X_test,y_train,y_test = train_test_split(df["filter_text"],df["Sentiment"],test_size=0.2,random_state=42)

y_test
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


max_words = 5000   # Vocabulary size
max_len = 100      # Sequence length

# Create tokenizer
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train.apply(lambda x: ' '.join(x)))

# Convert to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train.apply(lambda x: ' '.join(x)))
X_test_seq = tokenizer.texts_to_sequences(X_test.apply(lambda x: ' '.join(x)))

# Pad sequences
X_train = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

type(X_train)

from sklearn.preprocessing import LabelEncoder

#test = X_test.apply(lambda x:''.join(x))

label_encode = LabelEncoder()

y_train = label_encode.fit_transform(y_train)

y_train

y_test = label_encode.transform(y_test)

y_test

from keras import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
embedding_size = 32

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
import matplotlib.pyplot as plt

model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_size, input_length=max_len))

model.add(LSTM(100))
model.add(Dense(1, activation="sigmoid"))
model.summary()

model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

model.fit(X_train, y_train, batch_size=64, epochs=10)

def predict_sentiment(review):
           cleaned_review = re.sub('<.*?>', '', review)
           cleaned_review = re.sub(r'[^\\w\\s]', '', cleaned_review)
           cleaned_review = cleaned_review.lower()
           tokenized_review = word_tokenize(cleaned_review)
           filtered_review = [word for word in tokenized_review if word not in stop_words]
           lemmatized_review = [lamatize.lemmatize(word) for word in filtered_review]
           seq = tokenizer.texts_to_sequences([' '.join(lemmatized_review)]) # Changed stemmed_review to lemmatized_review
           padded = pad_sequences(seq, maxlen=max_len, padding='post')
           sentiment_prediction = model.predict(padded)

           if sentiment_prediction > 0.6:
               return "Positive"
           elif sentiment_prediction==0.6:
               return "Neutral"
           else:
               return "Negative"

review_to_predict = "This movie was amazing! I loved it."
predicted_sentiment = predict_sentiment(review_to_predict)
print("Predicted Sentiment:", predicted_sentiment)

"""Method 2

"""

!pip install transformers

from transformers import pipeline
sentiment_pipeline = pipeline("sentiment-analysis")
data = input("enter a text")
sentiment_pipeline(data)

